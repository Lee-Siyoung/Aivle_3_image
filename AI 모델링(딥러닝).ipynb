{"cells":[{"cell_type":"markdown","metadata":{"id":"Qyh6c5RHBVhU"},"source":["# 안녕하세요^^ \n","# AIVLE 미니 프로젝트에 오신 여러분을 환영합니다.\n","* 본 과정에서는 실제 사례와 데이터를 기반으로 문제를 해결하는 전체 과정을 자기 주도형 실습으로 진행해볼 예정입니다.\n","* 앞선 교육과정을 정리하는 마음과 지금까지 배운 내용을 바탕으로 문제 해결을 해볼게요!\n","* 미니 프로젝트를 통한 문제 해결 과정 'A에서 Z까지', 지금부터 시작합니다!"],"id":"Qyh6c5RHBVhU"},{"cell_type":"markdown","metadata":{"id":"1O-eCMb8BVhU"},"source":["---"],"id":"1O-eCMb8BVhU"},{"cell_type":"markdown","metadata":{"id":"EsTZabeTBVhV"},"source":["## 0. 환경 설정하기"],"id":"EsTZabeTBVhV"},{"cell_type":"markdown","source":["### 1) 구글 드라이브 연결하기"],"metadata":{"id":"xB7iJVzd7eHh"},"id":"xB7iJVzd7eHh"},{"cell_type":"code","source":["# 코랩 사용 시 구글 드라이브 연결\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"0yJHSX-HG6zY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664288914174,"user_tz":-540,"elapsed":19365,"user":{"displayName":"만보","userId":"16654482005967570253"}},"outputId":"f8c7dd87-b6a6-48aa-b07f-cab51c01cc74"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"id":"0yJHSX-HG6zY"},{"cell_type":"markdown","source":["### 2) 경로 확인하기\n","- \"WORK_SPACE\" 에 본인 작업 경로 작성 후 실행(구글 드라이브 최상위에 압축해제 시 그대로 실행. 수정 X).<br>\n","\n","<font color=\"red\">※ 주의. 나머지 경로는 절대 변경하지 마세요.</font>"],"metadata":{"id":"x0gyKH_wt8sH"},"id":"x0gyKH_wt8sH"},{"cell_type":"code","source":["# ROOT_PATH 확인 \n","import os\n","\n","# 구글 드라이브 내 프로젝트 압축해제된 영역 (구글 드라이브 최상위에 압축해제 시 그대로 실행 수정 X)\n","WORK_SPACE = \"kt aivle/mini3\"\n","\n","if os.getcwd() == '/content' :\n","  # 구글 드라이브 사용 시 \n","  ROOT_PATH = \"/content/drive/MyDrive/\"+WORK_SPACE+\"/AIVLE3rd_individual\"\n","else :\n","  ROOT_PATH = os.path.abspath('..')\n","# Train 데이터 셋 경로\n","TRAIN_PATH = ROOT_PATH + \"/train\"\n","# MODEL 저장 경로\n","MODEL_PATH = ROOT_PATH + \"/model\""],"metadata":{"id":"yjhDnP89VLjW","executionInfo":{"status":"ok","timestamp":1664288978847,"user_tz":-540,"elapsed":289,"user":{"displayName":"만보","userId":"16654482005967570253"}}},"execution_count":2,"outputs":[],"id":"yjhDnP89VLjW"},{"cell_type":"markdown","metadata":{"id":"r40YBXSQBVhX"},"source":["### 3) 라이브러리 불러오기\n","필요시 추가 라이브러리는 설치해서 사용하세요."],"id":"r40YBXSQBVhX"},{"cell_type":"code","execution_count":3,"metadata":{"scrolled":false,"id":"S3yEhuItBVhX","executionInfo":{"status":"ok","timestamp":1664288998853,"user_tz":-540,"elapsed":3270,"user":{"displayName":"만보","userId":"16654482005967570253"}}},"outputs":[],"source":["# 필요 라이브러리 불러오기.\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential, Model\n","from keras.layers import Input, Dense, Dropout, Activation, Embedding\n","from keras.layers import Conv2D, MaxPool2D, Flatten, BatchNormalization, Dropout\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.losses import categorical_crossentropy\n","from tensorflow.keras.backend import clear_session"],"id":"S3yEhuItBVhX"},{"cell_type":"markdown","source":["### 4) GPU 환경 확인하기\n","tensorflow가 GPU를 활용하고 있는지 확인하려면, tensorflow에서 제공하는 device_lib 라이브러리를 활용하면 됩니다."],"metadata":{"id":"VhHC2aku7pVO"},"id":"VhHC2aku7pVO"},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1MDovtfBVhY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664241415317,"user_tz":-540,"elapsed":11,"user":{"displayName":"만보","userId":"16654482005967570253"}},"outputId":"57ef7cec-2036-4106-c567-43b55b2b44ac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[name: \"/device:CPU:0\"\n"," device_type: \"CPU\"\n"," memory_limit: 268435456\n"," locality {\n"," }\n"," incarnation: 7103369379471590121\n"," xla_global_id: -1]"]},"metadata":{},"execution_count":4}],"source":["# GPU 환경 확인하기\n","from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()"],"id":"B1MDovtfBVhY"},{"cell_type":"markdown","metadata":{"id":"yaDDJVcfBVhZ"},"source":["---"],"id":"yaDDJVcfBVhZ"},{"cell_type":"markdown","metadata":{"id":"ynFemIXbBVhZ"},"source":["# 1. ImageDataGenerator 생성하기\n","앞의 <font color=\"red\">__'[study] 2.데이터전처리'__ </font>과정에서 사용하였던 ImageDataGenerator를 그대로 가져오시면 됩니다."],"id":"ynFemIXbBVhZ"},{"cell_type":"markdown","source":["<font color=\"green\">[실습문제]</font> 1. ImageDataGenerator 생성하기\n","+ 모델 검증을 위해 데이터를 train:validation(8:2)로 분할합니다.\n","+ 모델 성능 개선을 위해 데이터 증식(Data augmentation)이 필요 시 자유롭게 설정"],"metadata":{"id":"n4_qeqIxvSKY"},"id":"n4_qeqIxvSKY"},{"cell_type":"code","execution_count":6,"metadata":{"id":"F6Wa-ZChBVhZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664289111747,"user_tz":-540,"elapsed":298,"user":{"displayName":"만보","userId":"16654482005967570253"}},"outputId":"c0af2cec-f7ad-4406-e228-288f0b149a8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 867 images belonging to 4 classes.\n","Found 0 images belonging to 4 classes.\n"]}],"source":["# 실습해보세요.\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    #validation_split=0.2,\n","    rotation_range=90,  # 무작위 각도 범위\n","    width_shift_range=0.2,   # 수평 이동 범위 비율\n","    height_shift_range=0.2,  # 수직 이동 범위 비율\n","    shear_range=0.3,     # 층 밀리기 강도\n","    zoom_range=0.3,     # 무작위 줌 범위 \n","    horizontal_flip=True,  # 가로로 뒤집음\n","    # vertical_flip = True,    # 세로로 뒤집음 필요없을 듯 사람을 하다보니\n","    fill_mode='nearest',    # 변환된 이미지에서 생기는 빈 부분 채우는 방법\n",")\n","\n","val_datagen = ImageDataGenerator(\n","    rescale=1./255,\n",")\n","\n","batch_size=16\n","img_height = 480\n","img_width=854\n","\n","# train_genrator 생성\n","train_generator = train_datagen.flow_from_directory(\n","    TRAIN_PATH,\n","    batch_size=16,\n","    target_size=(img_height, img_width),\n","    class_mode='categorical',\n","    subset='training',\n","    shuffle = True\n",")\n","\n","# validation_generator 생성\n","validation_generator = val_datagen.flow_from_directory(\n","    TRAIN_PATH,\n","    batch_size=16,\n","    target_size=(img_height, img_width),\n","    subset='validation',\n","    class_mode='categorical',\n","    shuffle = False\n",")"],"id":"F6Wa-ZChBVhZ"},{"cell_type":"code","source":["print(train_generator.image_shape)\n","print(validation_generator.image_shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Le-Ru6WfV4G","executionInfo":{"status":"ok","timestamp":1664241425429,"user_tz":-540,"elapsed":441,"user":{"displayName":"만보","userId":"16654482005967570253"}},"outputId":"e10af30a-9468-41b5-b89c-470cfb8fd816"},"id":"9Le-Ru6WfV4G","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(480, 854, 3)\n","(480, 854, 3)\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"TdKYiWgHDPq0"},"id":"TdKYiWgHDPq0"},{"cell_type":"markdown","metadata":{"id":"LEQXf5oYBVhb"},"source":["# 2. 모델 구성하기\n","+ KeyPoint : 합성곱 신경망 (CNN) 모델 구성."],"id":"LEQXf5oYBVhb"},{"cell_type":"markdown","metadata":{"id":"NPbu10gwBVhb"},"source":["<font color=\"green\">[실습문제]</font> 2. CNN 모델을 설계해 보세요.\n","* 케라스를 이용해서 CNN 모델을 설계합니다."],"id":"NPbu10gwBVhb"},{"cell_type":"code","execution_count":7,"metadata":{"id":"EIdPj2faBVhb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664289139945,"user_tz":-540,"elapsed":4512,"user":{"displayName":"만보","userId":"16654482005967570253"}},"outputId":"07e16d0c-266b-4ee5-859f-ecbbb8017fc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 480, 854, 32)      896       \n","                                                                 \n"," batch_normalization (BatchN  (None, 480, 854, 32)     128       \n"," ormalization)                                                   \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 480, 854, 32)      9248      \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 480, 854, 32)     128       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 240, 427, 32)     0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 240, 427, 32)      0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 240, 427, 64)      18496     \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 240, 427, 64)     256       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 240, 427, 64)      36928     \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 240, 427, 64)     256       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 120, 213, 64)     0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 120, 213, 64)      0         \n","                                                                 \n"," flatten (Flatten)           (None, 1635840)           0         \n","                                                                 \n"," dense (Dense)               (None, 64)                104693824 \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 64)               256       \n"," hNormalization)                                                 \n","                                                                 \n"," dense_1 (Dense)             (None, 4)                 260       \n","                                                                 \n","=================================================================\n","Total params: 104,760,676\n","Trainable params: 104,760,164\n","Non-trainable params: 512\n","_________________________________________________________________\n"]}],"source":["clear_session()\n","model=Sequential()\n","\n","model.add(Input(shape=(480,854,3)))\n","model.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(64, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(4, activation='softmax'))\n","model.compile(loss=categorical_crossentropy, optimizer='adam', metrics=['accuracy'])\n","\n","model.summary()"],"id":"EIdPj2faBVhb"},{"cell_type":"markdown","metadata":{"id":"TjnCdnpUBVhc"},"source":["<font color=\"green\">[실습문제]</font> 3. 모델을 학습시켜 보세요.\n","* 위에서 설계한 모델에 데이터를 가지고 학습을 진행 합니다. \n","* history 변수에 학습 결과를 입력 받습니다.\n","* callback 함수로 ModelCheckpoint와 EarlyStopping을 사용하세요.(best만 저장)\n","* 학습한 모델의 weight를 경로 MODEL_PATH 에 저장해주세요.\n","* val_loss 기준으로 모니터링 해주세요."],"id":"TjnCdnpUBVhc"},{"cell_type":"code","execution_count":8,"metadata":{"id":"xjACm2WyBVhc","executionInfo":{"status":"ok","timestamp":1664289147117,"user_tz":-540,"elapsed":312,"user":{"displayName":"만보","userId":"16654482005967570253"}}},"outputs":[],"source":["# early_stopping \n","checkpoint = ModelCheckpoint(filepath='/content/drive/MyDrive/kt aivle/mini3/AIVLE3rd_individual/model/model.ckpt',\n","                      monitor='val_loss',\n","                      verbose=1,\n","                      save_best_only=True,\n","                      save_weights_only=False\n",")\n","  \n","\n","# early_stopping\n","early_stopping = EarlyStopping(monitor='val_loss',\n","                   min_delta=0,\n","                   patience=5,\n","                   verbose=1,\n","                   restore_best_weights=True\n",")"],"id":"xjACm2WyBVhc"},{"cell_type":"code","source":["# 모델 학습\n","history = model.fit(train_generator, validation_data=validation_generator\n","          ,epochs=130, verbose=1, callbacks=[early_stopping, checkpoint])\n"],"metadata":{"id":"z8F2pQr-nCKh","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1664296639837,"user_tz":-540,"elapsed":5429809,"user":{"displayName":"만보","userId":"16654482005967570253"}},"outputId":"b9ef953d-2860-4181-88eb-994aefd4eaba"},"id":"z8F2pQr-nCKh","execution_count":9,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/500\n","55/55 [==============================] - ETA: 0s - loss: 1.5776 - accuracy: 0.4602"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 326s 6s/step - loss: 1.5776 - accuracy: 0.4602\n","Epoch 2/500\n","55/55 [==============================] - ETA: 0s - loss: 1.1283 - accuracy: 0.5225"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 1.1283 - accuracy: 0.5225\n","Epoch 3/500\n","55/55 [==============================] - ETA: 0s - loss: 1.0086 - accuracy: 0.5525"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 1.0086 - accuracy: 0.5525\n","Epoch 4/500\n","55/55 [==============================] - ETA: 0s - loss: 1.0171 - accuracy: 0.5536"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 1.0171 - accuracy: 0.5536\n","Epoch 5/500\n","55/55 [==============================] - ETA: 0s - loss: 0.9390 - accuracy: 0.5675"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 0.9390 - accuracy: 0.5675\n","Epoch 6/500\n","55/55 [==============================] - ETA: 0s - loss: 0.9498 - accuracy: 0.5698"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 0.9498 - accuracy: 0.5698\n","Epoch 7/500\n","55/55 [==============================] - ETA: 0s - loss: 0.8853 - accuracy: 0.6148"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 0.8853 - accuracy: 0.6148\n","Epoch 8/500\n","55/55 [==============================] - ETA: 0s - loss: 0.8729 - accuracy: 0.6078"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 0.8729 - accuracy: 0.6078\n","Epoch 9/500\n","55/55 [==============================] - ETA: 0s - loss: 0.8707 - accuracy: 0.6032"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 0.8707 - accuracy: 0.6032\n","Epoch 10/500\n","55/55 [==============================] - ETA: 0s - loss: 0.8258 - accuracy: 0.6286"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 0.8258 - accuracy: 0.6286\n","Epoch 11/500\n","55/55 [==============================] - ETA: 0s - loss: 0.8303 - accuracy: 0.6113"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 79s 1s/step - loss: 0.8303 - accuracy: 0.6113\n","Epoch 12/500\n","55/55 [==============================] - ETA: 0s - loss: 0.8653 - accuracy: 0.6067"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 79s 1s/step - loss: 0.8653 - accuracy: 0.6067\n","Epoch 13/500\n","55/55 [==============================] - ETA: 0s - loss: 0.8234 - accuracy: 0.6321"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 0.8234 - accuracy: 0.6321\n","Epoch 14/500\n","55/55 [==============================] - ETA: 0s - loss: 0.9897 - accuracy: 0.5490"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 0.9897 - accuracy: 0.5490\n","Epoch 15/500\n","55/55 [==============================] - ETA: 0s - loss: 0.8835 - accuracy: 0.5986"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 0.8835 - accuracy: 0.5986\n","Epoch 16/500\n","55/55 [==============================] - ETA: 0s - loss: 0.8485 - accuracy: 0.6182"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 82s 1s/step - loss: 0.8485 - accuracy: 0.6182\n","Epoch 17/500\n","55/55 [==============================] - ETA: 0s - loss: 0.8286 - accuracy: 0.6205"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 82s 1s/step - loss: 0.8286 - accuracy: 0.6205\n","Epoch 18/500\n","55/55 [==============================] - ETA: 0s - loss: 0.7905 - accuracy: 0.6413"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 82s 1s/step - loss: 0.7905 - accuracy: 0.6413\n","Epoch 19/500\n","55/55 [==============================] - ETA: 0s - loss: 0.8290 - accuracy: 0.6286"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 81s 1s/step - loss: 0.8290 - accuracy: 0.6286\n","Epoch 20/500\n","55/55 [==============================] - ETA: 0s - loss: 0.7494 - accuracy: 0.6448"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 0.7494 - accuracy: 0.6448\n","Epoch 21/500\n","55/55 [==============================] - ETA: 0s - loss: 0.7712 - accuracy: 0.6632"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["55/55 [==============================] - 80s 1s/step - loss: 0.7712 - accuracy: 0.6632\n","Epoch 22/500\n","55/55 [==============================] - ETA: 0s - loss: 0.7401 - accuracy: 0.6286"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["55/55 [==============================] - 85s 2s/step - loss: 0.7401 - accuracy: 0.6286\n","Epoch 23/500\n","55/55 [==============================] - ETA: 0s - loss: 0.7245 - accuracy: 0.6690"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 85s 2s/step - loss: 0.7245 - accuracy: 0.6690\n","Epoch 24/500\n","55/55 [==============================] - ETA: 0s - loss: 0.7611 - accuracy: 0.6667"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 84s 2s/step - loss: 0.7611 - accuracy: 0.6667\n","Epoch 25/500\n","55/55 [==============================] - ETA: 0s - loss: 0.7230 - accuracy: 0.6482"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 82s 1s/step - loss: 0.7230 - accuracy: 0.6482\n","Epoch 26/500\n","55/55 [==============================] - ETA: 0s - loss: 0.7224 - accuracy: 0.6701"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.7224 - accuracy: 0.6701\n","Epoch 27/500\n","55/55 [==============================] - ETA: 0s - loss: 0.7085 - accuracy: 0.6747"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.7085 - accuracy: 0.6747\n","Epoch 28/500\n","55/55 [==============================] - ETA: 0s - loss: 0.7522 - accuracy: 0.6621"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.7522 - accuracy: 0.6621\n","Epoch 29/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6917 - accuracy: 0.6874"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 84s 2s/step - loss: 0.6917 - accuracy: 0.6874\n","Epoch 30/500\n","55/55 [==============================] - ETA: 0s - loss: 0.7036 - accuracy: 0.6517"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 82s 1s/step - loss: 0.7036 - accuracy: 0.6517\n","Epoch 31/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6843 - accuracy: 0.6621"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6843 - accuracy: 0.6621\n","Epoch 32/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6886 - accuracy: 0.6794"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6886 - accuracy: 0.6794\n","Epoch 33/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6307 - accuracy: 0.7036"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6307 - accuracy: 0.7036\n","Epoch 34/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6475 - accuracy: 0.6851"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6475 - accuracy: 0.6851\n","Epoch 35/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6571 - accuracy: 0.6828"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6571 - accuracy: 0.6828\n","Epoch 36/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6964 - accuracy: 0.6736"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6964 - accuracy: 0.6736\n","Epoch 37/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6680 - accuracy: 0.6794"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6680 - accuracy: 0.6794\n","Epoch 38/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6297 - accuracy: 0.6782"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6297 - accuracy: 0.6782\n","Epoch 39/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6006 - accuracy: 0.7163"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6006 - accuracy: 0.7163\n","Epoch 40/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6867 - accuracy: 0.6747"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6867 - accuracy: 0.6747\n","Epoch 41/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6561 - accuracy: 0.6747"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6561 - accuracy: 0.6747\n","Epoch 42/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6594 - accuracy: 0.6690"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6594 - accuracy: 0.6690\n","Epoch 43/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6538 - accuracy: 0.6990"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6538 - accuracy: 0.6990\n","Epoch 44/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6451 - accuracy: 0.6828"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6451 - accuracy: 0.6828\n","Epoch 45/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6321 - accuracy: 0.6943"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6321 - accuracy: 0.6943\n","Epoch 46/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6369 - accuracy: 0.6840"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6369 - accuracy: 0.6840\n","Epoch 47/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6263 - accuracy: 0.7070"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6263 - accuracy: 0.7070\n","Epoch 48/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6287 - accuracy: 0.6886"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6287 - accuracy: 0.6886\n","Epoch 49/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6128 - accuracy: 0.6909"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6128 - accuracy: 0.6909\n","Epoch 50/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6542 - accuracy: 0.6828"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6542 - accuracy: 0.6828\n","Epoch 51/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6627 - accuracy: 0.6805"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6627 - accuracy: 0.6805\n","Epoch 52/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6222 - accuracy: 0.6828"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6222 - accuracy: 0.6828\n","Epoch 53/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6238 - accuracy: 0.6817"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6238 - accuracy: 0.6817\n","Epoch 54/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6165 - accuracy: 0.7059"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6165 - accuracy: 0.7059\n","Epoch 55/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6126 - accuracy: 0.6817"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6126 - accuracy: 0.6817\n","Epoch 56/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6456 - accuracy: 0.6897"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6456 - accuracy: 0.6897\n","Epoch 57/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6041 - accuracy: 0.7128"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6041 - accuracy: 0.7128\n","Epoch 58/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6379 - accuracy: 0.7082"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6379 - accuracy: 0.7082\n","Epoch 59/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6148 - accuracy: 0.6920"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6148 - accuracy: 0.6920\n","Epoch 60/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5745 - accuracy: 0.7001"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5745 - accuracy: 0.7001\n","Epoch 61/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6163 - accuracy: 0.6909"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6163 - accuracy: 0.6909\n","Epoch 62/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6260 - accuracy: 0.7093"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6260 - accuracy: 0.7093\n","Epoch 63/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6306 - accuracy: 0.6863"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6306 - accuracy: 0.6863\n","Epoch 64/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5982 - accuracy: 0.7116"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5982 - accuracy: 0.7116\n","Epoch 65/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5832 - accuracy: 0.7059"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5832 - accuracy: 0.7059\n","Epoch 66/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.7209"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5822 - accuracy: 0.7209\n","Epoch 67/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6029 - accuracy: 0.7059"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6029 - accuracy: 0.7059\n","Epoch 68/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6355 - accuracy: 0.6817"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6355 - accuracy: 0.6817\n","Epoch 69/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6455 - accuracy: 0.6667"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.6455 - accuracy: 0.6667\n","Epoch 70/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5838 - accuracy: 0.6943"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5838 - accuracy: 0.6943\n","Epoch 71/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5539 - accuracy: 0.7405"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5539 - accuracy: 0.7405\n","Epoch 72/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5698 - accuracy: 0.7093"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5698 - accuracy: 0.7093\n","Epoch 73/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5850 - accuracy: 0.7001"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5850 - accuracy: 0.7001\n","Epoch 74/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5694 - accuracy: 0.7140"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5694 - accuracy: 0.7140\n","Epoch 75/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.6920"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5634 - accuracy: 0.6920\n","Epoch 76/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.7220"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5731 - accuracy: 0.7220\n","Epoch 77/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5991 - accuracy: 0.7001"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5991 - accuracy: 0.7001\n","Epoch 78/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5553 - accuracy: 0.7209"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.5553 - accuracy: 0.7209\n","Epoch 79/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5474 - accuracy: 0.7070"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.5474 - accuracy: 0.7070\n","Epoch 80/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5454 - accuracy: 0.7105"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.5454 - accuracy: 0.7105\n","Epoch 81/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.7105"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5621 - accuracy: 0.7105\n","Epoch 82/500\n","55/55 [==============================] - ETA: 0s - loss: 0.6235 - accuracy: 0.7047"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.6235 - accuracy: 0.7047\n","Epoch 83/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.7140"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.5664 - accuracy: 0.7140\n","Epoch 84/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.6990"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.5833 - accuracy: 0.6990\n","Epoch 85/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5594 - accuracy: 0.7266"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5594 - accuracy: 0.7266\n","Epoch 86/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.7174"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5460 - accuracy: 0.7174\n","Epoch 87/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.6886"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 79s 1s/step - loss: 0.5803 - accuracy: 0.6886\n","Epoch 88/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5534 - accuracy: 0.7197"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5534 - accuracy: 0.7197\n","Epoch 89/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.7209"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5466 - accuracy: 0.7209\n","Epoch 90/500\n","55/55 [==============================] - ETA: 0s - loss: 0.5511 - accuracy: 0.7059"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n","WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 80s 1s/step - loss: 0.5511 - accuracy: 0.7059\n","Epoch 91/500\n","20/55 [=========>....................] - ETA: 51s - loss: 0.5802 - accuracy: 0.6844"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-f20923954566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 모델 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(train_generator, validation_data=validation_generator\n\u001b[0;32m----> 3\u001b[0;31m           ,epochs=500, verbose=1, callbacks=[early_stopping, checkpoint])\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["<font color=\"green\">[실습문제]</font> 4. 모델 저장하기 \n","* 만들어진 모델를 기반으로 모델파일로 저장해주세요.\n","* 파일 저장 전에 ModelCheckpoint의 가중치(weights)를 로딩해주세요.\n","* 저장위치는 MODEL_PATH 입니다.\n","* 파일명은 <font color=\"red\">[개인] 미니프로젝트3차_A000000_OOO.h5</font>\n","\n","><font color=\"red\">[Hint]</font><br>\n",">모델 가중치는 load_weight 매소드로 불러옵니다.<br>\n",">모델 저장시에는 model.save 매소드를 사용합니다. "],"metadata":{"id":"QiG4-2_ZzvBF"},"id":"QiG4-2_ZzvBF"},{"cell_type":"code","source":["from google.colab import files\n","files.download('[개인]미니프로젝트3차_A023123_이시영.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"iWYBVZr80tAH","executionInfo":{"status":"ok","timestamp":1664246544708,"user_tz":-540,"elapsed":4,"user":{"displayName":"만보","userId":"16654482005967570253"}},"outputId":"7b77045c-fa14-49aa-e027-f5d5b3ecd44b"},"id":"iWYBVZr80tAH","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_54483a31-61b2-4500-81e0-99f8f069ef29\", \"[\\uac1c\\uc778]\\ubbf8\\ub2c8\\ud504\\ub85c\\uc81d\\ud2b83\\ucc28_A023123_\\uc774\\uc2dc\\uc601.h5\", 1257230760)"]},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Crjh1ZESBVhd"},"outputs":[],"source":["# 실습해보세요.\n","from keras.models import load_model\n","\n","model.save('[개인]미니프로젝트3차_A023123_이시영.h5')\n","\n","\n"],"id":"Crjh1ZESBVhd"},{"cell_type":"code","source":["from keras.models import load_model\n","model = load_model('[개인]미니프로젝트3차_A023123_이시영.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"seu6Kn75nkgY","executionInfo":{"status":"ok","timestamp":1664176275125,"user_tz":-540,"elapsed":3154,"user":{"displayName":"만보","userId":"16654482005967570253"}},"outputId":"2ce05ba4-bf4a-401b-9f6e-2fa9e31ef94d"},"id":"seu6Kn75nkgY","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<keras.metrics.Mean at 0x7fbfe78c30d0>,\n"," <keras.metrics.MeanMetricWrapper at 0x7fbfe7937b10>]"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"946631R-DRnz"},"id":"946631R-DRnz"},{"cell_type":"markdown","source":["# 3. 모델 평가하기"],"metadata":{"id":"EDC_S--HvYvc"},"id":"EDC_S--HvYvc"},{"cell_type":"markdown","metadata":{"id":"u52KNYYUqWtA"},"source":["<font color=\"green\">[실습문제]</font> 5. 훈련 과정에서 epoch에 따른 정확도와 손실을 시각화화여 확인합니다."],"id":"u52KNYYUqWtA"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5ZkI8ymBVhd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664178112141,"user_tz":-540,"elapsed":489,"user":{"displayName":"만보","userId":"16654482005967570253"}},"outputId":"0e36f16a-21d3-42ef-8e98-5312ac05e1cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.7510791420936584\n","0.7790697813034058\n","0.3304952383041382\n","0.7221317887306213\n"]}],"source":["# 실습해보세요.\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","print(history.history['accuracy'][-1])\n","print(history.history['val_accuracy'][-1])\n","print(history.history['loss'][-1])\n","print(history.history['val_loss'][-1])\n","\n","\n"],"id":"b5ZkI8ymBVhd"},{"cell_type":"code","source":[],"metadata":{"id":"cWuCqwPivyeC"},"id":"cWuCqwPivyeC","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color=\"green\">[실습문제]</font> 6. validation 데이터를 기준으로 학습한 모델을 적용해서 f1 Score를 계산하세요.\n","\n","* <font color=\"red\">hint.</font> sklearn.metrics 패키지에서 f1_score를 사용하세요."],"metadata":{"id":"9aPc69JKu5u5"},"id":"9aPc69JKu5u5"},{"cell_type":"code","source":["# 실습해보세요.\n","from sklearn.metrics import f1_score\n","import tensorflow as tf\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"mJlhPDydu4DN","colab":{"base_uri":"https://localhost:8080/","height":545},"executionInfo":{"status":"ok","timestamp":1664177235350,"user_tz":-540,"elapsed":835,"user":{"displayName":"만보","userId":"16654482005967570253"}},"outputId":"18a3be0f-c3cc-4683-b248-ab3558c44317"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"id":"mJlhPDydu4DN"},{"cell_type":"markdown","source":["---"],"metadata":{"id":"r9upu42cF10V"},"id":"r9upu42cF10V"},{"cell_type":"markdown","source":["## [추가학습] 모델 비교하기 \n","_시간이 남으면 해보세요._\n","\n","<font color=\"green\">[실습문제]</font> 7. 모델 구조를 변경해 보거나 다른 모델들을 만들어 보고 성능을 비교해 최고의 모델을 만들어 보세요.\n","- 여러분들이 배운 모델들을 다양하게 만들어 보고 성능을 비교해 보세요.\n","- ImageDataGenerator를 변경하면 성능 개선도 가능합니다. "],"metadata":{"id":"d9wESmIg-I5-"},"id":"d9wESmIg-I5-"},{"cell_type":"code","source":["# 실습해보세요.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"bxImTOBy-G7a"},"id":"bxImTOBy-G7a","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"machine_shape":"hm"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}